output:
  format: "json" # Maybe add option for pickle?
  dir: "./output/"
  file: "output.json"
warmup: False
warmup_options:
  requests: 11
  timeout_sec: 900
storage: # TODO
  type: local
dataset:
  file: "datasets/openorca_large_subset_011.jsonl" # or you can auto-generate using datasets/generate_dataset.py
  #use below params to filter the dataset
  max_queries: 1000 
  min_input_tokens: 0
  max_input_tokens: 10000
  max_output_tokens: 512
  max_sequence_tokens: 10000
  
load_options:
  type: constant #Future options: loadgen, stair-step
  concurrency: 2
  duration: 20 # In seconds. Maybe in future support "100s" "10m", etc...


# Azure MaaP (Deploy with your own provisioned compute)
#plugin: "azure_maap_plugin"
#plugin_options:
#  url: "https://phi-3-mini-128k-instruct-7.eastus2.inference.ml.azure.com/score"
#  key: ""
#  deployment: "phi-3-mini-128k-instruct-7"


# Azure Serverless (MaaS)
#plugin: "azure_serverless_plugin"
#plugin_options:
#  streaming: True
#  url: "https://<endpoint URI>/v1/chat/completions"  #dont forget to include /v1/chat/completions 
#  model_name: "Phi-3-medium-128k-instruct"
#  key: "<key>"

# Azure-OpenAI 
plugin: "azure_openai_plugin"
plugin_options:
  streaming: True
  url: "https://<endpoint>.openai.azure.com/"
  key: "<key>"
  deployment: "gpt-35-turbo"
  api_version: "2023-03-15-preview"


extra_metadata:
  replicas: 1
